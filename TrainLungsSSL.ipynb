{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73MAVDnhpxvu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from efficientunet import *\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "import os\n",
        "import kornia\n",
        "from kornia.augmentation import *\n",
        "from kornia.utils import get_cuda_or_mps_device_if_available, tensor_to_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XWsMdXlAgos"
      },
      "outputs": [],
      "source": [
        "def find_imgs(directory_path):\n",
        "    imgs = []\n",
        "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\"]  # Add more extensions as needed\n",
        "\n",
        "    def is_image(filename):\n",
        "        return any(filename.lower().endswith(extension) for extension in image_extensions)\n",
        "\n",
        "    for root, _, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if is_image(file):\n",
        "                image_path = os.path.join(root, file)\n",
        "                imgs.append(image_path)\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lG63yulG3tX"
      },
      "outputs": [],
      "source": [
        "lungs_path = '/gdrive/MyDrive/JustLungs'\n",
        "#len(find_imgs('/gdrive/MyDrive/JustLungs'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZam6VjIApGi"
      },
      "outputs": [],
      "source": [
        "class LungDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, img_paths, transforms):\n",
        "    self.img_paths = img_paths\n",
        "    self.transforms = transforms\n",
        "    self.img_type = kornia.io.ImageLoadType.GRAY32\n",
        "    self.randomperspective = RandomPerspective(0.3, \"nearest\", align_corners=True, same_on_batch=False,keepdim=True, p=0.5)\n",
        "    self.randomHorizontalflip = RandomHorizontalFlip(same_on_batch=False, keepdim=True, p=0.6, p_batch=0.5)\n",
        "    self.randomElastic = RandomElasticTransform(alpha=(0.3, 0.3), p=0.5, keepdim=True)\n",
        "    self.randomRotation = RandomRotation(degrees=20.0, p=0.5,keepdim=True)\n",
        "    self.randomJigsaw = RandomJigsaw((4, 4), p = 0.3, keepdim=True)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_org = kornia.io.load_image(self.img_paths[idx], self.img_type, device_k)\n",
        "    idx_neg = self.get_random_negative_index(idx)\n",
        "    x_neg = kornia.io.load_image(self.img_paths[idx_neg], self.img_type, device_k)\n",
        "    if self.transforms is not None:\n",
        "      img_org = self.transforms(img_org)\n",
        "      x_neg = self.transforms(x_neg)\n",
        "\n",
        "    # Good augmentation\n",
        "    x = self.apply_aug(img_org).squeeze(0)\n",
        "    x_neg = self.apply_ssl_aug(x_neg).squeeze(0)\n",
        "\n",
        "    # SSL Augmentations\n",
        "    x_pos = self.apply_ssl_aug(img_org).squeeze(0)\n",
        "\n",
        "    return torch.stack([x, x, x]), torch.stack([x_pos, x_pos, x_pos]), torch.stack([x_neg, x_neg, x_neg])\n",
        "\n",
        "  def get_random_negative_index(self, current_idx):\n",
        "    # Generate random indices excluding the current index\n",
        "    indices = torch.randperm(len(self.img_paths))\n",
        "    idx_neg = indices[indices != current_idx][0].item()\n",
        "    return idx_neg\n",
        "\n",
        "  def apply_aug(self, x):\n",
        "    x = self.randomperspective(x)\n",
        "    x = self.randomHorizontalflip(x)\n",
        "    x = self.randomElastic(x)\n",
        "    x = self.randomRotation(x)\n",
        "    return x\n",
        "\n",
        "  def apply_ssl_aug(self,x):\n",
        "    x = self.randomperspective(x)\n",
        "    x = self.randomHorizontalflip(x)\n",
        "    x = self.randomElastic(x)\n",
        "    x = self.randomRotation(x)\n",
        "    x = self.randomJigsaw(x)\n",
        "    return x\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOWSbUjwLIWB"
      },
      "outputs": [],
      "source": [
        "all_files = find_imgs(lungs_path)\n",
        "# logs (1)/\n",
        "bad_apples = np.load('/gdrive/MyDrive/logs (1)/bad_apple.npy')\n",
        "train_files = [element for element in all_files if element not in bad_apples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEMHUk3vUjcc"
      },
      "outputs": [],
      "source": [
        "lung_ds = LungDataset(train_files, transforms.Compose([transforms.Resize(224)]))\n",
        "# Define the proportions for the train and test sets\n",
        "train_size = int(0.9 * len(lung_ds))\n",
        "test_size = len(lung_ds) - train_size\n",
        "\n",
        "# Use random_split to create train and test datasets\n",
        "train_dataset, test_dataset = random_split(lung_ds, [train_size, test_size])\n",
        "\n",
        "# Create DataLoader instances for train and test datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xvB6RN1WGmj",
        "outputId": "709e82ab-30a6-48e3-c5d7-aea36e31c076"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b3-5fb5a3c3.pth\n",
            "100%|██████████| 47.1M/47.1M [00:00<00:00, 296MB/s]\n"
          ]
        }
      ],
      "source": [
        "b3unet = get_efficientunet_b3(out_channels=1, concat_input=True, pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydZbUX89nYIa"
      },
      "outputs": [],
      "source": [
        "class LinearLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 out_features,\n",
        "                 use_bias = True,\n",
        "                 use_bn = False,\n",
        "                 **kwargs):\n",
        "        super(LinearLayer, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "        self.use_bn = use_bn\n",
        "\n",
        "        self.linear = nn.Linear(self.in_features,\n",
        "                                self.out_features,\n",
        "                                bias = self.use_bias and not self.use_bn)\n",
        "        if self.use_bn:\n",
        "             self.bn = nn.BatchNorm1d(self.out_features)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.linear(x)\n",
        "        if self.use_bn:\n",
        "            x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "class ProjectionHead(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 hidden_features,\n",
        "                 out_features,\n",
        "                 head_type = 'nonlinear',\n",
        "                 **kwargs):\n",
        "        super(ProjectionHead,self).__init__(**kwargs)\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.hidden_features = hidden_features\n",
        "        self.head_type = head_type\n",
        "\n",
        "        if self.head_type == 'linear':\n",
        "            self.layers = LinearLayer(self.in_features,self.out_features,False, True)\n",
        "        elif self.head_type == 'nonlinear':\n",
        "            self.layers = nn.Sequential(\n",
        "                LinearLayer(self.in_features,self.hidden_features,True, True),\n",
        "                nn.ReLU(),\n",
        "                LinearLayer(self.hidden_features,self.out_features,False,True))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLTxpgXXc_PS"
      },
      "outputs": [],
      "source": [
        "class PreModel(torch.nn.Module):\n",
        "    def __init__(self,base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.conv_layer = nn.Conv2d(in_channels=1536, out_channels=120, kernel_size=(2,2), stride=(1,1), padding='same')\n",
        "        for p in self.base_model.parameters():\n",
        "            p.requires_grad = True\n",
        "        self.projector = ProjectionHead(5880, 2048, 128)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.base_model(x)\n",
        "        out = self.conv_layer(out)\n",
        "        out_flat = out.view(out.size(0), -1)\n",
        "        xp = self.projector(torch.squeeze(out_flat))\n",
        "\n",
        "        return xp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHyuAr2O2fnU",
        "outputId": "edfdcae2-250c-4823-b828-cc572e93b058"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVvrj-AcdHbG"
      },
      "outputs": [],
      "source": [
        "b3unet = get_efficientunet_b3(out_channels=1, concat_input=True, pretrained=True)\n",
        "model = PreModel(b3unet.encoder).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmGygENlcdAM"
      },
      "outputs": [],
      "source": [
        "triplet_loss = torch.nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYS3k-5IhRGh"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, current_epoch, name):\n",
        "    out = os.path.join('/gdrive/MyDrive/logs/',name.format(current_epoch))\n",
        "\n",
        "    torch.save({'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict':scheduler.state_dict()}, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rlUca8kkfIAe",
        "outputId": "0622ca11-13b6-46b5-e560-59f4ca434c0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/100]\t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [0/1654]\t Loss: 1.03532\n",
            "Step [50/1654]\t Loss: 1.95905\n",
            "Step [100/1654]\t Loss: 2.03513\n",
            "Step [150/1654]\t Loss: 1.16873\n",
            "Step [200/1654]\t Loss: 0.6047\n",
            "Step [250/1654]\t Loss: 0.36417\n",
            "Step [300/1654]\t Loss: 0.53508\n",
            "Step [350/1654]\t Loss: 0.49154\n",
            "Step [400/1654]\t Loss: 0.51618\n",
            "Step [450/1654]\t Loss: 0.39792\n",
            "Step [500/1654]\t Loss: 0.33009\n",
            "Step [550/1654]\t Loss: 0.15012\n",
            "Step [600/1654]\t Loss: 0.09468\n",
            "Step [650/1654]\t Loss: 0.23119\n",
            "Step [700/1654]\t Loss: 0.24123\n",
            "Step [750/1654]\t Loss: 0.06305\n",
            "Step [800/1654]\t Loss: 0.13805\n",
            "Step [850/1654]\t Loss: 0.07746\n",
            "Step [900/1654]\t Loss: 0.20462\n",
            "Step [950/1654]\t Loss: 0.0\n",
            "Step [1000/1654]\t Loss: 0.1018\n",
            "Step [1050/1654]\t Loss: 0.09122\n",
            "Step [1100/1654]\t Loss: 0.07124\n",
            "Step [1150/1654]\t Loss: 0.05503\n",
            "Step [1200/1654]\t Loss: 0.0\n",
            "Step [1250/1654]\t Loss: 0.19951\n",
            "Step [1300/1654]\t Loss: 0.17295\n",
            "Step [1350/1654]\t Loss: 0.17257\n",
            "Step [1400/1654]\t Loss: 0.35072\n",
            "Step [1450/1654]\t Loss: 0.0\n",
            "Step [1500/1654]\t Loss: 0.0\n",
            "Step [1550/1654]\t Loss: 0.06161\n",
            "Step [1600/1654]\t Loss: 0.26157\n",
            "Step [1650/1654]\t Loss: 0.08642\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [0/184]\t Loss: 0.15382\n",
            "Step [50/184]\t Loss: 0.00246\n",
            "Step [100/184]\t Loss: 0.0\n",
            "Step [150/184]\t Loss: 0.10393\n",
            "Epoch [0/100]\t Training Loss: 700.5687802694738\t lr: 0.01\n",
            "Epoch [0/100]\t Validation Loss: 33.07042724266648\t lr: 0.01\n",
            "Epoch [1/100]\t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [0/1654]\t Loss: 0.3435\n",
            "Step [50/1654]\t Loss: 0.00177\n",
            "Step [100/1654]\t Loss: 0.27973\n",
            "Step [150/1654]\t Loss: 0.03939\n",
            "Step [200/1654]\t Loss: 0.0\n",
            "Step [250/1654]\t Loss: 0.06831\n",
            "Step [300/1654]\t Loss: 0.0996\n",
            "Step [350/1654]\t Loss: 0.0\n",
            "Step [400/1654]\t Loss: 0.1331\n",
            "Step [450/1654]\t Loss: 0.01274\n",
            "Step [500/1654]\t Loss: 0.09353\n",
            "Step [550/1654]\t Loss: 0.05919\n",
            "Step [600/1654]\t Loss: 0.06387\n",
            "Step [650/1654]\t Loss: 0.11792\n",
            "Step [700/1654]\t Loss: 0.02001\n",
            "Step [750/1654]\t Loss: 0.07656\n",
            "Step [800/1654]\t Loss: 0.0\n",
            "Step [850/1654]\t Loss: 0.06752\n",
            "Step [900/1654]\t Loss: 0.15733\n",
            "Step [950/1654]\t Loss: 0.13729\n",
            "Step [1000/1654]\t Loss: 0.26864\n",
            "Step [1050/1654]\t Loss: 0.0\n",
            "Step [1100/1654]\t Loss: 0.02004\n",
            "Step [1150/1654]\t Loss: 0.01127\n",
            "Step [1200/1654]\t Loss: 0.06795\n",
            "Step [1250/1654]\t Loss: 0.05661\n",
            "Step [1300/1654]\t Loss: 0.32548\n",
            "Step [1350/1654]\t Loss: 0.01167\n",
            "Step [1400/1654]\t Loss: 0.0344\n",
            "Step [1450/1654]\t Loss: 0.35279\n",
            "Step [1500/1654]\t Loss: 0.14962\n",
            "Step [1550/1654]\t Loss: 0.09445\n",
            "Step [1600/1654]\t Loss: 0.0\n",
            "Step [1650/1654]\t Loss: 0.0876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [0/184]\t Loss: 0.0\n",
            "Step [50/184]\t Loss: 0.28295\n",
            "Step [100/184]\t Loss: 0.20475\n",
            "Step [150/184]\t Loss: 0.14781\n",
            "Epoch [1/100]\t Training Loss: 196.74497278407216\t lr: 0.01\n",
            "Epoch [1/100]\t Validation Loss: 29.646721355617046\t lr: 0.01\n",
            "Epoch [2/100]\t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [0/1654]\t Loss: 0.14764\n",
            "Step [50/1654]\t Loss: 0.63326\n",
            "Step [100/1654]\t Loss: 0.25923\n",
            "Step [150/1654]\t Loss: 0.10975\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-f57652894fb3>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtr_loss_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_n\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Caught Exception in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 364, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 364, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"<ipython-input-36-9bad853de4d1>\", line 13, in __getitem__\n    img_org = kornia.io.load_image(self.img_paths[idx], self.img_type, device_k)\n  File \"/usr/local/lib/python3.10/dist-packages/kornia/io/io.py\", line 75, in load_image\n    KORNIA_CHECK(os.path.isfile(path_file), f\"Invalid file: {path_file}\")\n  File \"/usr/local/lib/python3.10/dist-packages/kornia/core/check.py\", line 103, in KORNIA_CHECK\n    raise Exception(f\"{condition} not true.\\n{msg}\")\nException: False not true.\nInvalid file: /gdrive/MyDrive/JustLungs/20518.png\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "nr = 0\n",
        "current_epoch = 0\n",
        "epochs = 100\n",
        "tr_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for epoch in range(20):\n",
        "\n",
        "    print(f\"Epoch [{epoch}/{epochs}]\\t\")\n",
        "    stime = time.time()\n",
        "\n",
        "    model.train()\n",
        "    tr_loss_epoch = 0\n",
        "\n",
        "    for step, (x, x_p, x_n) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x = x.to(device).float()\n",
        "        x_p = x_p.to(device).float()\n",
        "        x_n = x_n.to(device).float()\n",
        "\n",
        "        # positive pair, with encoding\n",
        "        z_x = model(x)\n",
        "        z_xp = model(x_p)\n",
        "        z_xn = model(x_n)\n",
        "\n",
        "        loss = triplet_loss(z_x, z_xp, z_xn)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if nr == 0 and step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {round(loss.item(), 5)}\")\n",
        "            import matplotlib.pyplot as plt\n",
        "\n",
        "            # Display x, x_p, x_n\n",
        "            fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
        "\n",
        "            axs[0, 0].imshow(x[0].cpu().numpy().squeeze(), cmap='gray')\n",
        "            axs[0, 0].set_title('x')\n",
        "\n",
        "            axs[0, 1].imshow(x_p[0].cpu().numpy().squeeze(), cmap='gray')\n",
        "            axs[0, 1].set_title('x_p')\n",
        "\n",
        "            axs[0, 2].imshow(x_n[0].cpu().numpy().squeeze(), cmap='gray')\n",
        "            axs[0, 2].set_title('x_n')\n",
        "\n",
        "            # Display z_x, z_xp, z_xn\n",
        "            axs[1, 0].imshow(z_x[0].cpu().numpy().squeeze(), cmap='gray')\n",
        "            axs[1, 0].set_title('z_x')\n",
        "\n",
        "            axs[1, 1].imshow(z_xp[0].cpu().numpy().squeeze(), cmap='gray')\n",
        "            axs[1, 1].set_title('z_xp')\n",
        "\n",
        "            axs[1, 2].imshow(z_xn[0].cpu().numpy().squeeze(), cmap='gray')\n",
        "            axs[1, 2].set_title('z_xn')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        if nr == 0 and (epoch+1) % 5 == 0:\n",
        "          save_model(model, optimizer, scheduler, current_epoch,\"SSL_Chest_checkpoint_{}_260621.pt\")\n",
        "\n",
        "        tr_loss_epoch += loss.item()\n",
        "\n",
        "    lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss_epoch = 0\n",
        "        for step, (x, x_p, x_n) in enumerate(test_loader):\n",
        "\n",
        "          x = x.to(device).float()\n",
        "          x_p = x_p.to(device).float()\n",
        "          x_n = x_n.to(device).float()\n",
        "\n",
        "          # positive pair, with encoding\n",
        "          z_x = model(x)\n",
        "          z_xp = model(x_p)\n",
        "          z_xn = model(x_n)\n",
        "\n",
        "          loss = triplet_loss(z_x, z_xp, z_xn)\n",
        "\n",
        "          if nr == 0 and step % 50 == 0:\n",
        "              print(f\"Step [{step}/{len(test_loader)}]\\t Loss: {round(loss.item(),5)}\")\n",
        "\n",
        "          val_loss_epoch += loss.item()\n",
        "\n",
        "    if nr == 0:\n",
        "        tr_loss.append(tr_loss_epoch )\n",
        "        val_loss.append(val_loss_epoch )\n",
        "        print(f\"Epoch [{epoch}/{epochs}]\\t Training Loss: {tr_loss_epoch }\\t lr: {round(lr, 5)}\")\n",
        "        print(f\"Epoch [{epoch}/{epochs}]\\t Validation Loss: {val_loss_epoch }\\t lr: {round(lr, 5)}\")\n",
        "        current_epoch += 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
