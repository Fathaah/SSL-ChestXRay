{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_QRCOhINp3o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import torchxrayvision as xrv\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZVr9JeFc4aT",
        "outputId": "a982e841-f7cc-4923-f44b-2fdd1b179953"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-cf984f9c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-cf984f9c.pth\n",
            "100%|██████████| 47.2M/47.2M [00:00<00:00, 114MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models._api import WeightsEnum\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "def get_state_dict(self, *args, **kwargs):\n",
        "    kwargs.pop(\"check_hash\")\n",
        "    return load_state_dict_from_url(self.url, *args, **kwargs)\n",
        "WeightsEnum.get_state_dict = get_state_dict\n",
        "effNet = efficientnet_b3(weights = \"DEFAULT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wKLumNv5Vjo"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqJJhbeKvYEy"
      },
      "outputs": [],
      "source": [
        "class LungDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, img_paths, transforms):\n",
        "    self.img_paths = img_paths\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = read_image(self.img_paths[idx])\n",
        "    img = torch.tensor(img)\n",
        "    if self.transforms is not None:\n",
        "      img = self.transforms(img)\n",
        "    return img, self.img_paths[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRRTQE_nJgEl"
      },
      "outputs": [],
      "source": [
        "class JustGiveLungs(object):\n",
        "  def __init__(self, n = 5):\n",
        "    self.n = n\n",
        "    self.seg_model = xrv.baseline_models.chestx_det.PSPNet().to(device)\n",
        "\n",
        "  def __call__(self, img):\n",
        "\n",
        "    mask = self.getMasks(img)\n",
        "    return mask\n",
        "\n",
        "  def getMasks(self, img_org):\n",
        "    img_org = torch.moveaxis(img_org, 0, -1)\n",
        "    img1 = xrv.datasets.normalize(np.asarray(img_org), 255) # convert 8-bit image to [-1024, 1024] range\n",
        "    img1 = img1.mean(2)[None, ...] # Make single color channel\n",
        "    transform = torchvision.transforms.Compose([xrv.datasets.XRayCenterCrop(),xrv.datasets.XRayResizer(512, engine='cv2')])\n",
        "    img1 = transform(img1)\n",
        "    img = torch.from_numpy(img1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pred = self.seg_model(img.to(device))\n",
        "\n",
        "    pred = torch.sigmoid(pred)  # sigmoid\n",
        "    pred[pred < 0.5] = 0\n",
        "    pred[pred > 0.5] = 1\n",
        "\n",
        "    mask1 = self.pad_tensor_with_ones(pred[0, 4], self.n)\n",
        "    mask2 = self.pad_tensor_with_ones(pred[0, 5], self.n)\n",
        "\n",
        "    image_tensor = img_org.float()\n",
        "\n",
        "    # Resize masks to match the size of the input image\n",
        "    resize_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        # transforms.Resize((img_org.shape[0], img_org.shape[1])),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    mask1_resized = resize_transform(mask1)\n",
        "    mask2_resized = resize_transform(mask2)\n",
        "\n",
        "    # Create a combined mask where either mask1 or mask2 is 1\n",
        "    combined_mask = torch.max(mask1_resized, mask2_resized)\n",
        "\n",
        "    # Apply masks\n",
        "    combined_masked_image = image_tensor * torch.moveaxis(combined_mask, 0, -1)\n",
        "\n",
        "    nonzero_rows, nonzero_cols, _ = np.nonzero(combined_masked_image.numpy())\n",
        "    min_row, max_row = np.min(nonzero_rows), np.max(nonzero_rows)\n",
        "    min_col, max_col = np.min(nonzero_cols), np.max(nonzero_cols)\n",
        "    combined_masked_image_np = combined_masked_image[min_row:max_row + 1, min_col:max_col + 1, :]\n",
        "\n",
        "    return torch.moveaxis(combined_masked_image_np, 2, 0)\n",
        "\n",
        "  def pad_tensor_with_ones(self, input_tensor, n):\n",
        "    # Assuming input_tensor is a 2D tensor of shape [512, 512]\n",
        "    tensor = input_tensor.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "    kernel = torch.ones(1, 1, n, n, device=tensor.device)\n",
        "    padded_tensor = F.conv2d(tensor, kernel, padding=(n-1)//2)\n",
        "\n",
        "    padded_tensor = padded_tensor.squeeze(0).squeeze(0)\n",
        "    padded_tensor = (padded_tensor > 0).float()  # Convert non-zero values to 1\n",
        "\n",
        "    return padded_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6t0Fky66UG5"
      },
      "outputs": [],
      "source": [
        "def find_imgs(directory_path):\n",
        "    imgs = []\n",
        "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\"]  # Add more extensions as needed\n",
        "\n",
        "    def is_image(filename):\n",
        "        return any(filename.lower().endswith(extension) for extension in image_extensions)\n",
        "\n",
        "    for root, _, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if is_image(file):\n",
        "                image_path = os.path.join(root, file)\n",
        "                imgs.append(image_path)\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FtTiTzhbd1c"
      },
      "outputs": [],
      "source": [
        "class JustGiveLungsBatched(object):\n",
        "  def __init__(self, n = 5):\n",
        "    self.n = n\n",
        "    self.seg_model = xrv.baseline_models.chestx_det.PSPNet().to(device)\n",
        "\n",
        "  def __call__(self, img):\n",
        "\n",
        "    mask = self.getMasks(img)\n",
        "    return mask\n",
        "\n",
        "  def getMasks(self, img_orgs):\n",
        "\n",
        "    img_orgs = torch.moveaxis(img_orgs, 1, -1)\n",
        "    img1 = xrv.datasets.normalize(np.asarray(img_orgs), 255) # convert 8-bit image to [-1024, 1024] range\n",
        "    img1 = img1.mean(3)[None, ...] # Make single color channel\n",
        "    transform = torchvision.transforms.Compose([xrv.datasets.XRayCenterCrop(),xrv.datasets.XRayResizer(512, engine='cv2')])\n",
        "    img1 = transform(img1)\n",
        "    imgs.append((img1))\n",
        "\n",
        "    img = torch.from_numpy(np.asarray(imgs))\n",
        "    with torch.no_grad():\n",
        "      pred = self.seg_model(img.to(device))\n",
        "\n",
        "    pred = torch.sigmoid(pred)  # sigmoid\n",
        "    pred[pred < 0.5] = 0\n",
        "    pred[pred > 0.5] = 1\n",
        "\n",
        "    mask1 = self.pad_tensor_with_ones(pred[:, 4], self.n)\n",
        "    mask2 = self.pad_tensor_with_ones(pred[:, 5], self.n)\n",
        "\n",
        "    image_tensor = img_orgs.float()\n",
        "\n",
        "    # Resize masks to match the size of the input image\n",
        "    resize_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        # transforms.Resize((img_org.shape[0], img_org.shape[1])),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    mask1_resized = resize_transform(mask1)\n",
        "    mask2_resized = resize_transform(mask2)\n",
        "\n",
        "    # Create a combined mask where either mask1 or mask2 is 1\n",
        "    combined_mask = torch.max(mask1_resized, mask2_resized)\n",
        "\n",
        "    # Apply masks\n",
        "    combined_masked_image = image_tensor * torch.moveaxis(combined_mask, 0, -1)\n",
        "\n",
        "    nonzero_rows, nonzero_cols, _ = np.nonzero(combined_masked_image.numpy())\n",
        "    min_row, max_row = np.min(nonzero_rows), np.max(nonzero_rows)\n",
        "    min_col, max_col = np.min(nonzero_cols), np.max(nonzero_cols)\n",
        "    combined_masked_image_np = combined_masked_image[min_row:max_row + 1, min_col:max_col + 1, :]\n",
        "\n",
        "    return torch.moveaxis(combined_masked_image_np, 2, 0)\n",
        "\n",
        "  def pad_tensor_with_ones(self, input_tensor, n):\n",
        "    # Assuming input_tensor is a 2D tensor of shape [512, 512]\n",
        "    tensor = input_tensor.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "    kernel = torch.ones(1, 1, n, n, device=tensor.device)\n",
        "    padded_tensor = F.conv2d(tensor, kernel, padding=(n-1)//2)\n",
        "\n",
        "    padded_tensor = padded_tensor.squeeze(0).squeeze(0)\n",
        "    padded_tensor = (padded_tensor > 0).float()  # Convert non-zero values to 1\n",
        "\n",
        "    return padded_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXUXbRBN6i1W"
      },
      "outputs": [],
      "source": [
        "np.random.seed(10)\n",
        "cheXpert_imgs = find_imgs('/content/CheXpert-v1.0-small')\n",
        "cheXpert_s_imgs = list(np.random.choice(cheXpert_imgs, 15000))\n",
        "img_paths = cheXpert_s_imgs + find_imgs('/content/train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq1-tActAPHJ"
      },
      "outputs": [],
      "source": [
        "resize = transforms.Compose([\n",
        "    transforms.Resize([512, 512])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRS4Bust7FTT"
      },
      "outputs": [],
      "source": [
        "lung_ds = LungDataset(img_paths, resize)\n",
        "lung_dl = torch.utils.data.DataLoader(lung_ds, batch_size=32, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk437bHR0v9B"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "paths = []\n",
        "save_path = '/gdrive/MyDrive/JustLungs/'\n",
        "\n",
        "for idx,(img, path) in enumerate(lung_dl):\n",
        "  paths.append(path)\n",
        "  torchvision.utils.save_image(img[0] / 255, save_path+str(idx)+'.png')\n",
        "  clear_output(wait=True)\n",
        "  print(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Het5f8O70O9M"
      },
      "outputs": [],
      "source": [
        "class JustGiveLungsBatched(object):\n",
        "  def __init__(self, n = 5):\n",
        "    self.n = n\n",
        "    self.seg_model = xrv.baseline_models.chestx_det.PSPNet().to(device)\n",
        "\n",
        "  def __call__(self, img):\n",
        "\n",
        "    mask = self.getMasks(img)\n",
        "    return mask\n",
        "\n",
        "  def getMasks(self, img_orgs):\n",
        "\n",
        "    #img_orgs = torch.moveaxis(img_orgs, 1, -1)\n",
        "    img1 = (2 * (img_orgs / 255.0) - 1.0) * 1024 # convert 8-bit image to [-1024, 1024] range\n",
        "    imgs = img1.mean(1)[:,None, ...] # Make single color channel\n",
        "    print(imgs.shape)\n",
        "    with torch.no_grad():\n",
        "      pred = self.seg_model(imgs)\n",
        "\n",
        "    pred = torch.sigmoid(pred)  # sigmoid\n",
        "    pred[pred < 0.5] = 0\n",
        "    pred[pred > 0.5] = 1\n",
        "\n",
        "    mask1 = self.pad_tensor_with_ones(pred[:, 4], self.n)\n",
        "    mask2 = self.pad_tensor_with_ones(pred[:, 5], self.n)\n",
        "\n",
        "    image_tensor = img_orgs.float()\n",
        "\n",
        "    mask1_resized = (mask1)\n",
        "    mask2_resized = (mask2)\n",
        "\n",
        "    # Create a combined mask where either mask1 or mask2 is 1\n",
        "    combined_mask = torch.max(mask1_resized, mask2_resized)\n",
        "\n",
        "    # Apply masks\n",
        "    combined_masked_image = image_tensor * combined_mask\n",
        "\n",
        "    return combined_masked_image\n",
        "\n",
        "  def pad_tensor_with_ones(self, input_tensor, n):\n",
        "    # Assuming input_tensor is a 2D tensor of shape [512, 512]\n",
        "    tensor = input_tensor.unsqueeze(1)  # Add batch and channel dimensions\n",
        "    kernel = torch.ones(1, 1, n, n, device=tensor.device)\n",
        "    padded_tensor = F.conv2d(tensor, kernel, padding=(n-1)//2)\n",
        "\n",
        "    padded_tensor = padded_tensor.squeeze(0).squeeze(0)\n",
        "    padded_tensor = (padded_tensor > 0).float()  # Convert non-zero values to 1\n",
        "\n",
        "    return padded_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svnG-V06BxVy",
        "outputId": "179c422b-1bc7-4ac6-db4d-293d490b9231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "paths = []\n",
        "save_path = '/gdrive/MyDrive/JustLungs/'\n",
        "\n",
        "idx = 0\n",
        "for (imgs, path) in lung_dl:\n",
        "  paths = paths + list(path)\n",
        "  imgs = imgs.cpu().cuda()\n",
        "  out = gib.getMasks(imgs)\n",
        "  for i in range(out.shape[0]):\n",
        "    torchvision.utils.save_image(out[i] / 255, save_path+str(idx)+'.png')\n",
        "    idx += 1\n",
        "  clear_output(wait=True)\n",
        "  print(idx / 30000.0 )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
